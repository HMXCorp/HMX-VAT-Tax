{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\r\n",
        "from delta.tables import *\r\n",
        "import re\r\n",
        "import json"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "raw_folderpath = \"raw/.../\"\r\n",
        "raw_filename = \"\"\r\n",
        "primary_key_cols = '[\"column1\",\"column2\", \"etc\"]'\r\n",
        "partition_cols = '[\"CalcYear\",\"column2\", \"etc\"]'\r\n",
        "date_partition_column = 'DATECOLUMNNAME'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert parameter partition_cols from string type to list type\r\n",
        "partition_cols_list = json.loads(partition_cols)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set to Correct Database"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "CREATE DATABASE IF NOT EXISTS staging;\r\n",
        "USE staging;"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "0",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-05-16T22:07:06.4083678Z",
              "session_start_time": null,
              "execution_start_time": "2023-05-16T22:09:54.1237096Z",
              "execution_finish_time": "2023-05-16T22:09:54.1238893Z",
              "spark_jobs": null,
              "parent_msg_id": "e485bfd5-e44a-497b-ad4e-594bf55d7db2"
            },
            "text/plain": "StatementMeta(, 0, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": []
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 0 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamically Get Storage Account Name From Linked Service"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storageLinkedService = 'LS_DataLake'\r\n",
        "storageAccount_ls = mssparkutils.credentials.getPropertiesAll(storageLinkedService)\r\n",
        "storageAccountName = json.loads(storageAccount_ls)['Endpoint'].split('.')[0].replace('https://','')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Input Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_folderpathWithoutContainer = raw_folderpath[4:]\r\n",
        "\r\n",
        "raw_data = spark.read.load(f'abfss://raw@{storageAccountName}.dfs.core.windows.net/{storageLinkedService}{raw_filename}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Date Partition Columns If Necessary"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'CalcYear' in partition_cols_list:\r\n",
        "    raw_data = raw_data.withColumn('CalcYear', f.year(f.col(date_partition_column)))\r\n",
        "\r\n",
        "if 'CalcMonth' in partition_cols_list:\r\n",
        "    raw_data = raw_data.withColumn('CalcMonth', f.month(f.col(date_partition_column)))\r\n",
        "\r\n",
        "if 'CalcDayOfMonth' in partition_cols_list:\r\n",
        "    raw_data = raw_data.withColumn('CalcDayOfMonth', f.dayofmonth(f.col(date_partition_column)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Output Path"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stagingFolderPath = re.sub(r'(.*/v[0-9]+/).*', r'\\1', raw_folderpathWithoutContainer)\r\n",
        "stagingAbfssPath = f'abfss://staging@{storageAccountName}.dfs.core.windows.net/{stagingFolderPath}'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine if Delta Table Exists"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tableName = '_'.join(stagingFolderPath.split('/')[:-1])\r\n",
        "\r\n",
        "tableExists = spark.catalog.tableExists(tableName)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If table does **not** exist, create new delta table\r\n",
        "- Either with or without partitions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tableExists == False:\r\n",
        "    if partition_cols == \"[]\":\r\n",
        "        #output with no partition specified\r\n",
        "        raw_data.write.format(\"delta\") \\\r\n",
        "                .option(\"path\", stagingAbfssPath) \\\r\n",
        "                .saveAsTable(tableName)\r\n",
        "    else:\r\n",
        "        # output with specified partition\r\n",
        "        raw_data.write.format(\"delta\") \\\r\n",
        "                .partitionBy(partition_cols_list) \\\r\n",
        "                .option(\"path\", stagingAbfssPath) \\\r\n",
        "                .saveAsTable(tableName)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format primary key columns for merge statement"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert parameter primary_key_cols from string type to list type\r\n",
        "primary_key_cols_list = json.loads(primary_key_cols)\r\n",
        "\r\n",
        "# set initial mergeOn statement using first primary key columns\r\n",
        "mergeOn = f'current.{primary_key_cols_list[0]} = new.{primary_key_cols_list[0]}'\r\n",
        "\r\n",
        "# Add additional primary key columns to string with preceeding AND\r\n",
        "for primary_key_col in primary_key_cols_list[1:]:\r\n",
        "    mergeOn = mergeOn + f' AND current.{primary_key_col} = new.{primary_key_col}'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If Table Exists Merge New Data Into Existing Delta Table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tableExists == True:\r\n",
        "    deltaTablePointer = DeltaTable.forPath(spark, stagingAbfssPath)\r\n",
        "\r\n",
        "    deltaTablePointer.alias(\"current\").merge(\r\n",
        "        raw_data.alias(\"new\"), f\"{mergeOn}\" ) \\\r\n",
        "        .whenMatchedUpdateAll() \\\r\n",
        "        .whenNotMatchedInsertAll() \\\r\n",
        "        .execute()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}